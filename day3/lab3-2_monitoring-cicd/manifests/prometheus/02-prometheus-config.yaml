apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s

    # Alertmanager configuration
    alerting:
      alertmanagers:
        - static_configs:
            - targets:
                - alertmanager.monitoring.svc.cluster.local:9093

    # Alert rules
    rule_files:
      - /etc/prometheus/rules/*.yml

    # Scrape configurations
    scrape_configs:
      # Prometheus self-monitoring
      - job_name: 'prometheus'
        static_configs:
          - targets: ['localhost:9090']

      # KServe InferenceService metrics
      - job_name: 'kserve-models'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - kubeflow-user01
                - kubeflow-user02
                - kubeflow-user03
                - kubeflow-user04
                - kubeflow-user05
                - kubeflow-user06
                - kubeflow-user07
                - kubeflow-user08
                - kubeflow-user09
                - kubeflow-user10
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_serving_kserve_io_inferenceservice]
            action: keep
            regex: (.+)
          - source_labels: [__meta_kubernetes_pod_container_port_name]
            action: keep
            regex: metrics
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_pod_label_serving_kserve_io_inferenceservice]
            target_label: model_name

      # Custom model metrics exporter
      - job_name: 'metrics-exporter'
        static_configs:
          - targets: ['metrics-exporter.monitoring.svc.cluster.local:8000']
        scrape_interval: 15s
        scrape_timeout: 10s

      # Node exporter (system metrics)
      - job_name: 'node-exporter'
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - source_labels: [__address__]
            regex: '(.*):10250'
            replacement: '${1}:9100'
            target_label: __address__

      # Kubelet metrics
      - job_name: 'kubelet'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)

  alert_rules.yml: |
    groups:
      - name: model_performance_alerts
        interval: 30s
        rules:
          # Model MAE threshold alert
          - alert: ModelPerformanceDegraded
            expr: model_mae_score > 0.40
            for: 5m
            labels:
              severity: warning
              team: ml-platform
            annotations:
              summary: "Model performance degraded"
              description: "Model {{ $labels.model_name }} MAE ({{ $value | humanize }}) exceeded threshold 0.40 for 5 minutes"

          # High prediction latency alert
          - alert: ModelLatencyHigh
            expr: histogram_quantile(0.95, rate(model_prediction_latency_bucket[5m])) > 0.1
            for: 5m
            labels:
              severity: warning
              team: ml-platform
            annotations:
              summary: "Model prediction latency too high"
              description: "Model {{ $labels.model_name }} 95th percentile latency ({{ $value | humanize }}s) exceeded 100ms"

          # High error rate alert
          - alert: ModelErrorRateHigh
            expr: rate(model_prediction_errors_total[5m]) > 0.01
            for: 5m
            labels:
              severity: critical
              team: ml-platform
            annotations:
              summary: "Model error rate too high"
              description: "Model {{ $labels.model_name }} error rate ({{ $value | humanizePercentage }}) exceeded 1%"

          # Model accuracy drop alert
          - alert: ModelAccuracyDropped
            expr: model_accuracy_score < 0.75
            for: 10m
            labels:
              severity: warning
              team: ml-platform
            annotations:
              summary: "Model accuracy dropped significantly"
              description: "Model {{ $labels.model_name }} accuracy ({{ $value | humanize }}) dropped below 75%"

          # A/B test performance difference alert
          - alert: ABTestPerformanceDifference
            expr: |
              abs(
                model_mae_score{version="v1.0"} - 
                model_mae_score{version="v2.0"}
              ) > 0.05
            for: 10m
            labels:
              severity: info
              team: ml-platform
            annotations:
              summary: "Significant A/B test performance difference detected"
              description: "MAE difference between v1.0 and v2.0 exceeded 0.05 for 10 minutes"

      - name: infrastructure_alerts
        interval: 30s
        rules:
          # Pod not ready alert
          - alert: ModelPodNotReady
            expr: kube_pod_status_ready{condition="true", namespace=~"kubeflow-user.*"} == 0
            for: 5m
            labels:
              severity: warning
              team: ml-platform
            annotations:
              summary: "Model pod not ready"
              description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has been not ready for 5 minutes"

          # High CPU usage alert
          - alert: ModelHighCPUUsage
            expr: rate(container_cpu_usage_seconds_total{namespace=~"kubeflow-user.*"}[5m]) > 0.8
            for: 5m
            labels:
              severity: warning
              team: ml-platform
            annotations:
              summary: "Model container high CPU usage"
              description: "Container {{ $labels.container }} in pod {{ $labels.pod }} CPU usage ({{ $value | humanizePercentage }}) exceeded 80%"

          # High memory usage alert
          - alert: ModelHighMemoryUsage
            expr: container_memory_usage_bytes{namespace=~"kubeflow-user.*"} / container_spec_memory_limit_bytes > 0.9
            for: 5m
            labels:
              severity: warning
              team: ml-platform
            annotations:
              summary: "Model container high memory usage"
              description: "Container {{ $labels.container }} in pod {{ $labels.pod }} memory usage ({{ $value | humanizePercentage }}) exceeded 90%"
