"""
Lab 3-2: ëª¨ë¸ ë°°í¬ ì»´í¬ë„ŒíŠ¸
===========================

KServe InferenceServiceë¡œ ëª¨ë¸ì„ ë°°í¬í•˜ëŠ” ì»´í¬ë„ŒíŠ¸

í˜„ëŒ€ì˜¤í† ì—ë²„ MLOps Training
"""

from kfp import dsl
from kfp.dsl import component


@component(
    base_image="python:3.9-slim",
    packages_to_install=["kubernetes==28.1.0", "mlflow==2.9.2"]
)
def deploy_model(
    run_id: str,
    model_name: str,
    namespace: str,
    mlflow_tracking_uri: str
):
    """
    KServe InferenceServiceë¡œ ëª¨ë¸ ë°°í¬
    
    Args:
        run_id: MLflow Run ID
        model_name: ëª¨ë¸/InferenceService ì´ë¦„
        namespace: Kubernetes ë„¤ì„ìŠ¤í˜ì´ìŠ¤
        mlflow_tracking_uri: MLflow ì„œë²„ URI
    """
    from kubernetes import client, config
    from kubernetes.client.rest import ApiException
    import time
    import os
    
    print("=" * 60)
    print("  Component: Deploy Model (KServe)")
    print("=" * 60)
    
    print(f"\n  Configuration:")
    print(f"     - Model Name: {model_name}")
    print(f"     - Namespace: {namespace}")
    print(f"     - Run ID: {run_id}")
    print(f"     - MLflow URI: {mlflow_tracking_uri}")
    
    # Kubernetes í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
    try:
        config.load_incluster_config()
        print(f"\n  âœ… Using in-cluster config")
    except:
        config.load_kube_config()
        print(f"\n  âœ… Using kubeconfig")
    
    api = client.CustomObjectsApi()
    
    # InferenceService ì •ì˜
    model_uri = f"mlflow-artifacts:/{run_id}/model"
    
    isvc = {
        "apiVersion": "serving.kserve.io/v1beta1",
        "kind": "InferenceService",
        "metadata": {
            "name": model_name,
            "namespace": namespace,
            "annotations": {
                "sidecar.istio.io/inject": "false"
            }
        },
        "spec": {
            "predictor": {
                "sklearn": {
                    "storageUri": model_uri,
                    "resources": {
                        "requests": {
                            "cpu": "100m",
                            "memory": "256Mi"
                        },
                        "limits": {
                            "cpu": "500m",
                            "memory": "512Mi"
                        }
                    }
                }
            }
        }
    }
    
    # ê¸°ì¡´ InferenceService ì‚­ì œ (ìˆìœ¼ë©´)
    print(f"\n  ğŸ—‘ï¸ Checking existing InferenceService...")
    try:
        api.delete_namespaced_custom_object(
            group="serving.kserve.io",
            version="v1beta1",
            namespace=namespace,
            plural="inferenceservices",
            name=model_name
        )
        print(f"  âœ… Deleted existing InferenceService: {model_name}")
        time.sleep(5)
    except ApiException as e:
        if e.status == 404:
            print(f"  âœ… No existing InferenceService found")
        else:
            raise
    
    # ìƒˆë¡œ ìƒì„±
    print(f"\n  ğŸš€ Creating InferenceService...")
    try:
        result = api.create_namespaced_custom_object(
            group="serving.kserve.io",
            version="v1beta1",
            namespace=namespace,
            plural="inferenceservices",
            body=isvc
        )
        print(f"  âœ… InferenceService created: {model_name}")
        
    except ApiException as e:
        print(f"  âŒ Failed to create InferenceService: {e.reason}")
        raise
    
    # ìƒíƒœ í™•ì¸
    print(f"\n  â³ Waiting for deployment (max 60s)...")
    for i in range(6):
        time.sleep(10)
        try:
            isvc_status = api.get_namespaced_custom_object(
                group="serving.kserve.io",
                version="v1beta1",
                namespace=namespace,
                plural="inferenceservices",
                name=model_name
            )
            
            conditions = isvc_status.get("status", {}).get("conditions", [])
            ready_condition = next(
                (c for c in conditions if c.get("type") == "Ready"),
                None
            )
            
            if ready_condition and ready_condition.get("status") == "True":
                print(f"  âœ… InferenceService READY!")
                break
            else:
                status = ready_condition.get("status", "Unknown") if ready_condition else "Unknown"
                message = ready_condition.get("message", "") if ready_condition else ""
                print(f"  â³ Status: {status} ({(i+1)*10}s)")
                if message:
                    print(f"      Message: {message[:50]}...")
                
        except Exception as e:
            print(f"  âš ï¸ Status check failed: {e}")
    
    # ì—”ë“œí¬ì¸íŠ¸ ì •ë³´
    internal_url = f"http://{model_name}.{namespace}.svc.cluster.local/v1/models/{model_name}:predict"
    
    print(f"\n  ğŸ“¡ Deployment Information:")
    print(f"     - Internal URL: {internal_url}")
    print(f"\n  ğŸ§ª Test Command:")
    print(f"""     curl -X POST {internal_url} \\
       -H 'Content-Type: application/json' \\
       -d '{{"instances": [[0.5, 0.2, -0.1, -0.15, 0.3, 0.1, 0.8, -0.5]]}}'""")
    
    print(f"\n  âœ… Deployment completed!")


# ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    print("This component requires Kubernetes cluster to run.")
    print("Please use within Kubeflow Pipeline.")
