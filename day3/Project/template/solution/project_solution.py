"""
Lab 3-2: ÌîÑÎ°úÏ†ùÌä∏ ÏÜîÎ£®ÏÖò (ÏòàÏ†ú)
================================

‚ö†Ô∏è Ïù¥ ÌååÏùºÏùÄ Î∞úÌëú ÌõÑÏóê Í≥µÍ∞úÎê©ÎãàÎã§.
ÌåÄ ÌîÑÎ°úÏ†ùÌä∏ ÏôÑÏÑ± ÏòàÏ†úÏûÖÎãàÎã§.

ÌòÑÎåÄÏò§ÌÜ†ÏóêÎ≤Ñ MLOps Training
"""

import os
from kfp import dsl
from kfp.dsl import component, Input, Output, Dataset, Model
from kfp import compiler


# ============================================================
# ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï
# ============================================================
TEAM_NAME = os.environ.get("TEAM_NAME", "solution-team")
USER_NAMESPACE = os.environ.get("NAMESPACE", "kubeflow-user01")
MLFLOW_TRACKING_URI = os.environ.get(
    "MLFLOW_TRACKING_URI",
    "http://mlflow-server-service.mlflow-system.svc.cluster.local:5000"
)


# ============================================================
# Component 1: Îç∞Ïù¥ÌÑ∞ Î°úÎìú
# ============================================================
@component(
    base_image="python:3.9-slim",
    packages_to_install=["pandas==2.0.3", "scikit-learn==1.3.2"]
)
def load_data(
    dataset_name: str,
    output_data: Output[Dataset]
):
    """California Housing Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú"""
    import pandas as pd
    from sklearn.datasets import fetch_california_housing
    
    print("=" * 60)
    print("  Step 1: Load Data")
    print("=" * 60)
    
    housing = fetch_california_housing(as_frame=True)
    df = housing.frame
    
    print(f"\n  Dataset: {dataset_name}")
    print(f"  Shape: {df.shape}")
    print(f"  Columns: {list(df.columns)}")
    print(f"\n  Target statistics:")
    print(f"    Mean: {df['MedHouseVal'].mean():.4f}")
    print(f"    Std: {df['MedHouseVal'].std():.4f}")
    print(f"    Min: {df['MedHouseVal'].min():.4f}")
    print(f"    Max: {df['MedHouseVal'].max():.4f}")
    
    df.to_csv(output_data.path, index=False)
    print(f"\n  ‚úÖ Data saved: {output_data.path}")


# ============================================================
# Component 2: Ï†ÑÏ≤òÎ¶¨
# ============================================================
@component(
    base_image="python:3.9-slim",
    packages_to_install=["pandas==2.0.3", "scikit-learn==1.3.2", "numpy==1.24.3", "joblib==1.3.2"]
)
def preprocess(
    input_data: Input[Dataset],
    X_train_out: Output[Dataset],
    X_test_out: Output[Dataset],
    y_train_out: Output[Dataset],
    y_test_out: Output[Dataset],
    scaler_out: Output[Model],
    test_size: float = 0.2
) -> dict:
    """Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨: Î∂ÑÌï† Î∞è Ï†ïÍ∑úÌôî"""
    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    import joblib
    
    print("=" * 60)
    print("  Step 2: Preprocess")
    print("=" * 60)
    
    df = pd.read_csv(input_data.path)
    print(f"\n  Loaded {len(df)} rows")
    
    # ÌîºÏ≤òÏôÄ ÌÉÄÍ≤ü Î∂ÑÎ¶¨
    X = df.drop(columns=['MedHouseVal'])
    y = df['MedHouseVal']
    
    print(f"  Features: {list(X.columns)}")
    
    # Train/Test Î∂ÑÌï†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42
    )
    
    print(f"\n  Train/Test Split:")
    print(f"    Train: {len(X_train)} ({(1-test_size)*100:.0f}%)")
    print(f"    Test: {len(X_test)} ({test_size*100:.0f}%)")
    
    # Ï†ïÍ∑úÌôî
    scaler = StandardScaler()
    X_train_scaled = pd.DataFrame(
        scaler.fit_transform(X_train),
        columns=X_train.columns
    )
    X_test_scaled = pd.DataFrame(
        scaler.transform(X_test),
        columns=X_test.columns
    )
    
    # Ï†ÄÏû•
    X_train_scaled.to_csv(X_train_out.path, index=False)
    X_test_scaled.to_csv(X_test_out.path, index=False)
    y_train.to_csv(y_train_out.path, index=False)
    y_test.to_csv(y_test_out.path, index=False)
    joblib.dump(scaler, scaler_out.path)
    
    print(f"\n  ‚úÖ Preprocessing completed")
    
    return {
        "n_train": len(X_train),
        "n_test": len(X_test),
        "n_features": X_train.shape[1]
    }


# ============================================================
# Component 3: ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ (ÏôÑÏÑ± Î≤ÑÏ†Ñ)
# ============================================================
@component(
    base_image="python:3.9-slim",
    packages_to_install=["pandas==2.0.3", "numpy==1.24.3"]
)
def feature_engineering(
    X_train_in: Input[Dataset],
    X_test_in: Input[Dataset],
    X_train_out: Output[Dataset],
    X_test_out: Output[Dataset]
) -> dict:
    """
    ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ - ÏôÑÏÑ± Î≤ÑÏ†Ñ
    
    ÏÉùÏÑ±ÎêòÎäî ÌîºÏ≤ò:
    1. rooms_per_household: Í∞ÄÍµ¨Îãπ Î∞© Ïàò
    2. bedrooms_ratio: Î∞© ÎåÄÎπÑ Ïπ®Ïã§ ÎπÑÏú®
    3. population_per_household: Í∞ÄÍµ¨Îãπ Ïù∏Íµ¨
    4. location_score: ÏúÑÏπò Ï†êÏàò
    5. density: Î∞ÄÏßëÎèÑ ÏßÄÌëú
    6. income_rooms_interaction: ÏÜåÎìù √ó Î∞© Ïàò ÏÉÅÌò∏ÏûëÏö©
    """
    import pandas as pd
    import numpy as np
    
    print("=" * 60)
    print("  Step 3: Feature Engineering")
    print("=" * 60)
    
    X_train = pd.read_csv(X_train_in.path)
    X_test = pd.read_csv(X_test_in.path)
    
    original_features = list(X_train.columns)
    print(f"\n  Original features ({len(original_features)}):")
    for feat in original_features:
        print(f"    - {feat}")
    
    def add_features(df):
        """ÌååÏÉù Î≥ÄÏàò Ï∂îÍ∞Ä"""
        df = df.copy()
        
        # 1. Í∞ÄÍµ¨Îãπ Î∞© Ïàò
        df['rooms_per_household'] = df['AveRooms'] / (df['AveOccup'] + 1e-6)
        
        # 2. Î∞© ÎåÄÎπÑ Ïπ®Ïã§ ÎπÑÏú®
        df['bedrooms_ratio'] = df['AveBedrms'] / (df['AveRooms'] + 1e-6)
        
        # 3. Í∞ÄÍµ¨Îãπ Ïù∏Íµ¨
        df['population_per_household'] = df['Population'] / (df['AveOccup'] + 1e-6)
        
        # 4. ÏúÑÏπò Ï†êÏàò (Ï†ïÍ∑úÌôîÎêú Ï¢åÌëú Í∏∞Î∞ò)
        df['location_score'] = np.sqrt(
            df['Latitude']**2 + df['Longitude']**2
        )
        
        # 5. Î∞ÄÏßëÎèÑ ÏßÄÌëú
        df['density'] = df['Population'] * df['AveOccup']
        
        # 6. ÏÜåÎìùÍ≥º Î∞© ÏàòÏùò ÏÉÅÌò∏ÏûëÏö©
        df['income_rooms_interaction'] = df['MedInc'] * df['AveRooms']
        
        # 7. Ï£ºÌÉù Ïó∞Î†π Í∑∏Î£π (Î≤îÏ£ºÌòï ‚Üí ÏàòÏπòÌòï)
        df['house_age_group'] = pd.cut(
            df['HouseAge'], 
            bins=[-np.inf, -0.5, 0, 0.5, np.inf], 
            labels=[1, 2, 3, 4]
        ).astype(float)
        
        return df
    
    X_train_fe = add_features(X_train)
    X_test_fe = add_features(X_test)
    
    new_features = [f for f in X_train_fe.columns if f not in original_features]
    
    print(f"\n  New features ({len(new_features)}):")
    for feat in new_features:
        stats = X_train_fe[feat].describe()
        print(f"    - {feat}: mean={stats['mean']:.4f}, std={stats['std']:.4f}")
    
    print(f"\n  Total features: {len(X_train_fe.columns)}")
    
    X_train_fe.to_csv(X_train_out.path, index=False)
    X_test_fe.to_csv(X_test_out.path, index=False)
    
    print(f"\n  ‚úÖ Feature engineering completed")
    
    return {
        "original_features": len(original_features),
        "new_features": len(new_features),
        "total_features": len(X_train_fe.columns),
        "new_feature_names": new_features
    }


# ============================================================
# Component 4: Î™®Îç∏ ÌïôÏäµ (ÏôÑÏÑ± Î≤ÑÏ†Ñ)
# ============================================================
@component(
    base_image="python:3.9-slim",
    packages_to_install=[
        "pandas==2.0.3",
        "scikit-learn==1.3.2",
        "mlflow==2.9.2",
        "numpy==1.24.3",
        "boto3==1.34.0"
    ]
)
def train_model(
    X_train: Input[Dataset],
    X_test: Input[Dataset],
    y_train: Input[Dataset],
    y_test: Input[Dataset],
    mlflow_tracking_uri: str,
    experiment_name: str,
    team_name: str,
    n_estimators: int = 100,
    max_depth: int = 10
) -> str:
    """Î™®Îç∏ ÌïôÏäµ Î∞è MLflow Í∏∞Î°ù"""
    import pandas as pd
    import numpy as np
    import mlflow
    import mlflow.sklearn
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
    import os
    
    print("=" * 60)
    print(f"  Step 4: Model Training - {team_name}")
    print("=" * 60)
    
    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    X_train_df = pd.read_csv(X_train.path)
    X_test_df = pd.read_csv(X_test.path)
    y_train_df = pd.read_csv(y_train.path)
    y_test_df = pd.read_csv(y_test.path)
    
    print(f"\n  Data shapes:")
    print(f"    X_train: {X_train_df.shape}")
    print(f"    X_test: {X_test_df.shape}")
    
    # MLflow ÏÑ§Ï†ï
    os.environ['MLFLOW_TRACKING_URI'] = mlflow_tracking_uri
    mlflow.set_tracking_uri(mlflow_tracking_uri)
    mlflow.set_experiment(experiment_name)
    
    print(f"\n  MLflow Configuration:")
    print(f"    URI: {mlflow_tracking_uri}")
    print(f"    Experiment: {experiment_name}")
    
    with mlflow.start_run(run_name=f"{team_name}-run") as run:
        run_id = run.info.run_id
        print(f"\n  Run ID: {run_id}")
        
        # ÌååÎùºÎØ∏ÌÑ∞ Î°úÍπÖ
        params = {
            "n_estimators": n_estimators,
            "max_depth": max_depth,
            "random_state": 42,
            "n_jobs": -1,
            "model_type": "RandomForestRegressor"
        }
        mlflow.log_params(params)
        mlflow.set_tag("team", team_name)
        mlflow.set_tag("pipeline", "e2e-project")
        
        # Î™®Îç∏ ÌïôÏäµ
        print(f"\n  Training RandomForest...")
        model = RandomForestRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=42,
            n_jobs=-1
        )
        model.fit(X_train_df, y_train_df.values.ravel())
        
        # ÏòàÏ∏° Î∞è ÌèâÍ∞Ä
        y_pred = model.predict(X_test_df)
        
        r2 = r2_score(y_test_df, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test_df, y_pred))
        mae = mean_absolute_error(y_test_df, y_pred)
        mse = mean_squared_error(y_test_df, y_pred)
        
        # Î©îÌä∏Î¶≠ Î°úÍπÖ
        metrics = {"r2": r2, "rmse": rmse, "mae": mae, "mse": mse}
        mlflow.log_metrics(metrics)
        
        print(f"\n  Model Performance:")
        print(f"    R2 Score: {r2:.4f}")
        print(f"    RMSE: {rmse:.4f}")
        print(f"    MAE: {mae:.4f}")
        
        # ÌîºÏ≤ò Ï§ëÏöîÎèÑ Î°úÍπÖ
        feature_importance = dict(zip(
            X_train_df.columns,
            model.feature_importances_
        ))
        mlflow.log_dict(feature_importance, "feature_importance.json")
        
        print(f"\n  Top 5 Important Features:")
        sorted_importance = sorted(
            feature_importance.items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]
        for i, (feat, imp) in enumerate(sorted_importance, 1):
            print(f"    {i}. {feat}: {imp:.4f}")
        
        # Î™®Îç∏ Ï†ÄÏû•
        mlflow.sklearn.log_model(
            model,
            "model",
            registered_model_name=f"{team_name}-california-model"
        )
        
        print(f"\n  ‚úÖ Model training completed")
    
    return run_id


# ============================================================
# Component 5: Î™®Îç∏ ÌèâÍ∞Ä
# ============================================================
@component(
    base_image="python:3.9-slim",
    packages_to_install=["mlflow==2.9.2"]
)
def evaluate_model(
    run_id: str,
    mlflow_tracking_uri: str,
    r2_threshold: float = 0.75
) -> str:
    """Î™®Îç∏ ÌèâÍ∞Ä Î∞è Î∞∞Ìè¨ Í≤∞Ï†ï"""
    import mlflow
    import os
    
    print("=" * 60)
    print("  Step 5: Model Evaluation")
    print("=" * 60)
    
    os.environ['MLFLOW_TRACKING_URI'] = mlflow_tracking_uri
    mlflow.set_tracking_uri(mlflow_tracking_uri)
    
    client = mlflow.tracking.MlflowClient()
    run = client.get_run(run_id)
    
    r2 = float(run.data.metrics.get("r2", 0))
    rmse = float(run.data.metrics.get("rmse", 0))
    mae = float(run.data.metrics.get("mae", 0))
    
    print(f"\n  Run ID: {run_id}")
    print(f"\n  Model Metrics:")
    print(f"    R2 Score: {r2:.4f}")
    print(f"    RMSE: {rmse:.4f}")
    print(f"    MAE: {mae:.4f}")
    print(f"\n  Deployment Threshold: R2 >= {r2_threshold}")
    
    if r2 >= r2_threshold:
        decision = "deploy"
        print(f"\n  ‚úÖ Decision: DEPLOY")
        print(f"     Reason: R2 ({r2:.4f}) >= Threshold ({r2_threshold})")
    else:
        decision = "skip"
        print(f"\n  ‚ö†Ô∏è Decision: SKIP")
        print(f"     Reason: R2 ({r2:.4f}) < Threshold ({r2_threshold})")
    
    with mlflow.start_run(run_id=run_id):
        mlflow.set_tag("deployment_decision", decision)
    
    return decision


# ============================================================
# Component 6: Î™®Îç∏ Î∞∞Ìè¨
# ============================================================
@component(
    base_image="python:3.9-slim",
    packages_to_install=["kubernetes==28.1.0", "mlflow==2.9.2"]
)
def deploy_model(
    run_id: str,
    model_name: str,
    namespace: str,
    mlflow_tracking_uri: str
):
    """KServe InferenceServiceÎ°ú Î™®Îç∏ Î∞∞Ìè¨"""
    from kubernetes import client, config
    from kubernetes.client.rest import ApiException
    import time
    
    print("=" * 60)
    print("  Step 6: Model Deployment (KServe)")
    print("=" * 60)
    
    print(f"\n  Configuration:")
    print(f"    Model Name: {model_name}")
    print(f"    Namespace: {namespace}")
    print(f"    Run ID: {run_id}")
    
    try:
        config.load_incluster_config()
    except:
        config.load_kube_config()
    
    api = client.CustomObjectsApi()
    
    isvc = {
        "apiVersion": "serving.kserve.io/v1beta1",
        "kind": "InferenceService",
        "metadata": {
            "name": model_name,
            "namespace": namespace,
            "annotations": {
                "sidecar.istio.io/inject": "false"
            }
        },
        "spec": {
            "predictor": {
                "sklearn": {
                    "storageUri": f"mlflow-artifacts:/{run_id}/model",
                    "resources": {
                        "requests": {"cpu": "100m", "memory": "256Mi"},
                        "limits": {"cpu": "500m", "memory": "512Mi"}
                    }
                }
            }
        }
    }
    
    # Í∏∞Ï°¥ ÏÇ≠Ï†ú
    try:
        api.delete_namespaced_custom_object(
            group="serving.kserve.io",
            version="v1beta1",
            namespace=namespace,
            plural="inferenceservices",
            name=model_name
        )
        print(f"\n  Deleted existing InferenceService")
        time.sleep(5)
    except ApiException as e:
        if e.status != 404:
            raise
    
    # ÏÉùÏÑ±
    api.create_namespaced_custom_object(
        group="serving.kserve.io",
        version="v1beta1",
        namespace=namespace,
        plural="inferenceservices",
        body=isvc
    )
    
    print(f"\n  ‚úÖ InferenceService created: {model_name}")
    
    # ÏÉÅÌÉú ÌôïÏù∏
    print(f"\n  Waiting for deployment...")
    for i in range(6):
        time.sleep(10)
        try:
            status = api.get_namespaced_custom_object(
                group="serving.kserve.io",
                version="v1beta1",
                namespace=namespace,
                plural="inferenceservices",
                name=model_name
            )
            conditions = status.get("status", {}).get("conditions", [])
            ready = next((c for c in conditions if c.get("type") == "Ready"), None)
            if ready and ready.get("status") == "True":
                print(f"  ‚úÖ InferenceService READY!")
                break
            print(f"  ‚è≥ Status: {ready.get('status') if ready else 'Unknown'} ({(i+1)*10}s)")
        except:
            pass
    
    print(f"\n  Endpoint:")
    print(f"    http://{model_name}.{namespace}.svc.cluster.local/v1/models/{model_name}:predict")
    print(f"\n  ‚úÖ Deployment completed!")


# ============================================================
# Component 7: ÏïåÎ¶º
# ============================================================
@component(base_image="python:3.9-slim")
def send_alert(run_id: str, team_name: str):
    """ÏÑ±Îä• ÎØ∏Îã¨ ÏïåÎ¶º"""
    print("=" * 60)
    print(f"  Alert - {team_name}")
    print("=" * 60)
    print(f"\n  ‚ö†Ô∏è Model did not meet performance threshold")
    print(f"  Run ID: {run_id}")
    print(f"\n  Recommendations:")
    print(f"    1. Add more training data")
    print(f"    2. Create additional features")
    print(f"    3. Tune hyperparameters")
    print(f"    4. Try different algorithms")


# ============================================================
# ÌååÏù¥ÌîÑÎùºÏù∏ Ï†ïÏùò
# ============================================================
@dsl.pipeline(
    name="Project Pipeline (Solution)",
    description="Team Project Solution: Complete E2E ML Pipeline"
)
def project_pipeline(
    dataset_name: str = "california",
    team_name: str = "solution-team",
    experiment_name: str = "solution-experiment",
    model_name: str = "solution-model",
    namespace: str = "kubeflow-user01",
    mlflow_tracking_uri: str = "http://mlflow-server-service.mlflow-system.svc.cluster.local:5000",
    n_estimators: int = 100,
    max_depth: int = 10,
    r2_threshold: float = 0.75
):
    """ÌîÑÎ°úÏ†ùÌä∏ ÏÜîÎ£®ÏÖò ÌååÏù¥ÌîÑÎùºÏù∏"""
    
    # Step 1: Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    load_task = load_data(dataset_name=dataset_name)
    
    # Step 2: Ï†ÑÏ≤òÎ¶¨
    preprocess_task = preprocess(input_data=load_task.outputs["output_data"])
    
    # Step 3: ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ
    feature_task = feature_engineering(
        X_train_in=preprocess_task.outputs["X_train_out"],
        X_test_in=preprocess_task.outputs["X_test_out"]
    )
    
    # Step 4: Î™®Îç∏ ÌïôÏäµ
    train_task = train_model(
        X_train=feature_task.outputs["X_train_out"],
        X_test=feature_task.outputs["X_test_out"],
        y_train=preprocess_task.outputs["y_train_out"],
        y_test=preprocess_task.outputs["y_test_out"],
        mlflow_tracking_uri=mlflow_tracking_uri,
        experiment_name=experiment_name,
        team_name=team_name,
        n_estimators=n_estimators,
        max_depth=max_depth
    )
    
    # Step 5: ÌèâÍ∞Ä
    evaluate_task = evaluate_model(
        run_id=train_task.output,
        mlflow_tracking_uri=mlflow_tracking_uri,
        r2_threshold=r2_threshold
    )
    
    # Step 6: Ï°∞Í±¥Î∂Ä Î∞∞Ìè¨
    with dsl.If(evaluate_task.output == "deploy"):
        deploy_model(
            run_id=train_task.output,
            model_name=model_name,
            namespace=namespace,
            mlflow_tracking_uri=mlflow_tracking_uri
        )
    
    with dsl.If(evaluate_task.output == "skip"):
        send_alert(run_id=train_task.output, team_name=team_name)


# ============================================================
# Main
# ============================================================
if __name__ == "__main__":
    print("=" * 60)
    print("  Project Pipeline Solution")
    print("=" * 60)
    
    pipeline_file = "project_solution_pipeline.yaml"
    
    compiler.Compiler().compile(
        pipeline_func=project_pipeline,
        package_path=pipeline_file
    )
    
    print(f"\n‚úÖ Pipeline compiled: {pipeline_file}")
    print(f"\nüìã This solution includes:")
    print(f"  - 7 new engineered features")
    print(f"  - Complete MLflow integration")
    print(f"  - Feature importance logging")
    print(f"  - KServe deployment with status check")
    print("=" * 60)
