{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ Lab 3-2: ì¡°ë³„ í”„ë¡œì íŠ¸ (Part 2)\n",
    "\n",
    "## í”„ë¡œì íŠ¸ ê°œìš”\n",
    "\n",
    "ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ **ì™„ì „í•œ E2E MLOps íŒŒì´í”„ë¼ì¸**ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í‰ê°€ ê¸°ì¤€\n",
    "\n",
    "| í•­ëª© | ë°°ì  | ì„¤ëª… |\n",
    "|------|------|------|\n",
    "| Kubeflow Pipeline | 40ì  | ìµœì†Œ 5ê°œ ì»´í¬ë„ŒíŠ¸, Succeeded ìƒíƒœ |\n",
    "| MLflow Tracking | 20ì  | ìµœì†Œ 2íšŒ Run, íŒŒë¼ë¯¸í„°/ë©”íŠ¸ë¦­ ê¸°ë¡ |\n",
    "| Feature Engineering | 10ì  | 1ê°œ ì´ìƒ íŒŒìƒ í”¼ì²˜ ìƒì„± |\n",
    "| KServe ë°°í¬ | 25ì  | InferenceService ìƒì„± ë° API í…ŒìŠ¤íŠ¸ |\n",
    "| Canary ë°°í¬ | 5ì  (ë³´ë„ˆìŠ¤) | íŠ¸ë˜í”½ ë¶„í•  ì ìš© |\n",
    "\n",
    "### íŒŒì´í”„ë¼ì¸ êµ¬ì¡°\n",
    "\n",
    "```\n",
    "load_data â†’ preprocess â†’ feature_engineering â†’ train_model â†’ evaluate â†’ deploy/alert\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install kfp>=2.0.0 mlflow>=2.9.0 scikit-learn pandas numpy -q\n",
    "print(\"âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import os\n",
    "from kfp import dsl\n",
    "from kfp.dsl import component, Input, Output, Dataset, Model\n",
    "from kfp import compiler\n",
    "import kfp\n",
    "\n",
    "print(f\"KFP Version: {kfp.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš ï¸ TODO: íŒ€ ì„¤ì •ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”!\n",
    "TEAM_NAME = \"team-XX\"           # ì˜ˆ: team-01, team-02, ...\n",
    "USER_NAMESPACE = \"kubeflow-user01\"  # ë³¸ì¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤\n",
    "\n",
    "MLFLOW_TRACKING_URI = os.getenv(\n",
    "    'MLFLOW_TRACKING_URI',\n",
    "    'http://mlflow-server-service.mlflow-system.svc.cluster.local:5000'\n",
    ")\n",
    "\n",
    "print(f\"Team: {TEAM_NAME}\")\n",
    "print(f\"Namespace: {USER_NAMESPACE}\")\n",
    "print(f\"MLflow URI: {MLFLOW_TRACKING_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. ì»´í¬ë„ŒíŠ¸ êµ¬í˜„\n",
    "\n",
    "### 2.1 ë°ì´í„° ë¡œë“œ ì»´í¬ë„ŒíŠ¸\n",
    "\n",
    "**TODO**: ë°ì´í„° ë¡œë“œ ì»´í¬ë„ŒíŠ¸ë¥¼ ì™„ì„±í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"pandas==2.0.3\", \"scikit-learn==1.3.2\"]\n",
    ")\n",
    "def load_data(dataset_name: str, output_data: Output[Dataset]):\n",
    "    \"\"\"\n",
    "    California Housing ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "    \n",
    "    TODO:\n",
    "    1. sklearnì—ì„œ California Housing ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "    2. DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "    3. CSVë¡œ ì €ì¥\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.datasets import fetch_california_housing\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"  Loading {dataset_name} dataset\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # TODO: ë°ì´í„° ë¡œë“œ êµ¬í˜„\n",
    "    housing = fetch_california_housing(as_frame=True)\n",
    "    df = housing.frame\n",
    "    \n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # TODO: CSV ì €ì¥\n",
    "    df.to_csv(output_data.path, index=False)\n",
    "    print(f\"  âœ… Saved to: {output_data.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ì „ì²˜ë¦¬ ì»´í¬ë„ŒíŠ¸\n",
    "\n",
    "**TODO**: ì „ì²˜ë¦¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ì™„ì„±í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"pandas==2.0.3\", \"scikit-learn==1.3.2\", \"numpy==1.24.3\"]\n",
    ")\n",
    "def preprocess(\n",
    "    input_data: Input[Dataset],\n",
    "    X_train_out: Output[Dataset],\n",
    "    X_test_out: Output[Dataset],\n",
    "    y_train_out: Output[Dataset],\n",
    "    y_test_out: Output[Dataset],\n",
    "    test_size: float = 0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    ë°ì´í„° ì „ì²˜ë¦¬\n",
    "    \n",
    "    TODO:\n",
    "    1. CSV ë¡œë“œ\n",
    "    2. í”¼ì²˜/íƒ€ê²Ÿ ë¶„ë¦¬\n",
    "    3. Train/Test ë¶„í• \n",
    "    4. StandardScalerë¡œ ì •ê·œí™”\n",
    "    5. ì €ì¥\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"  Preprocessing\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df = pd.read_csv(input_data.path)\n",
    "    \n",
    "    # TODO: í”¼ì²˜/íƒ€ê²Ÿ ë¶„ë¦¬\n",
    "    X = df.drop(columns=['MedHouseVal'])\n",
    "    y = df['MedHouseVal']\n",
    "    \n",
    "    # TODO: Train/Test ë¶„í• \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    # TODO: ì •ê·œí™”\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train),\n",
    "        columns=X_train.columns\n",
    "    )\n",
    "    X_test_scaled = pd.DataFrame(\n",
    "        scaler.transform(X_test),\n",
    "        columns=X_test.columns\n",
    "    )\n",
    "    \n",
    "    # TODO: ì €ì¥\n",
    "    X_train_scaled.to_csv(X_train_out.path, index=False)\n",
    "    X_test_scaled.to_csv(X_test_out.path, index=False)\n",
    "    y_train.to_csv(y_train_out.path, index=False)\n",
    "    y_test.to_csv(y_test_out.path, index=False)\n",
    "    \n",
    "    print(f\"  Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "    print(f\"  âœ… Preprocessing completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì»´í¬ë„ŒíŠ¸ â­\n",
    "\n",
    "**TODO**: íŒ€ì—ì„œ ì°½ì˜ì ì¸ í”¼ì²˜ë¥¼ ì¶”ê°€í•˜ì„¸ìš”! (í‰ê°€ í•­ëª©)\n",
    "\n",
    "#### í”¼ì²˜ ì•„ì´ë””ì–´\n",
    "\n",
    "```python\n",
    "# ì•„ì´ë””ì–´ 1: ë°©ë‹¹ ì¹¨ì‹¤ ë¹„ìœ¨\n",
    "df['bedroom_ratio'] = df['AveBedrms'] / df['AveRooms']\n",
    "\n",
    "# ì•„ì´ë””ì–´ 2: ê°€êµ¬ë‹¹ ì¸êµ¬\n",
    "df['people_per_household'] = df['Population'] / df['AveOccup']\n",
    "\n",
    "# ì•„ì´ë””ì–´ 3: Bay Areaê¹Œì§€ ê±°ë¦¬\n",
    "bay_area_lat, bay_area_long = 37.77, -122.42\n",
    "df['dist_to_bay'] = np.sqrt(\n",
    "    (df['Latitude'] - bay_area_lat)**2 + \n",
    "    (df['Longitude'] - bay_area_long)**2\n",
    ")\n",
    "\n",
    "# ì•„ì´ë””ì–´ 4: ì†Œë“ êµ¬ê°„\n",
    "df['income_category'] = pd.cut(df['MedInc'], bins=5, labels=[1,2,3,4,5])\n",
    "\n",
    "# ì•„ì´ë””ì–´ 5: ë°€ì§‘ë„\n",
    "df['density'] = df['Population'] * df['AveOccup']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"pandas==2.0.3\", \"numpy==1.24.3\"]\n",
    ")\n",
    "def feature_engineering(\n",
    "    X_train_in: Input[Dataset],\n",
    "    X_test_in: Input[Dataset],\n",
    "    X_train_out: Output[Dataset],\n",
    "    X_test_out: Output[Dataset]\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
    "    \n",
    "    TODO: íŒ€ì—ì„œ ì°½ì˜ì ì¸ í”¼ì²˜ë¥¼ ì¶”ê°€í•˜ì„¸ìš”!\n",
    "    ìµœì†Œ 1ê°œ ì´ìƒì˜ íŒŒìƒ í”¼ì²˜ í•„ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        ìƒì„±ëœ ìƒˆ í”¼ì²˜ ê°œìˆ˜\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"  Feature Engineering\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    X_train = pd.read_csv(X_train_in.path)\n",
    "    X_test = pd.read_csv(X_test_in.path)\n",
    "    \n",
    "    original_cols = list(X_train.columns)\n",
    "    \n",
    "    def add_features(df):\n",
    "        \"\"\"TODO: íŒŒìƒ ë³€ìˆ˜ ì¶”ê°€\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # TODO: ì—¬ê¸°ì— ìƒˆë¡œìš´ í”¼ì²˜ë¥¼ ì¶”ê°€í•˜ì„¸ìš”!\n",
    "        # ì˜ˆì‹œ:\n",
    "        df['bedroom_ratio'] = df['AveBedrms'] / (df['AveRooms'] + 1e-6)\n",
    "        df['people_per_household'] = df['Population'] / (df['AveOccup'] + 1e-6)\n",
    "        \n",
    "        # TODO: ë” ë§ì€ í”¼ì²˜ ì¶”ê°€...\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    X_train_fe = add_features(X_train)\n",
    "    X_test_fe = add_features(X_test)\n",
    "    \n",
    "    new_cols = [c for c in X_train_fe.columns if c not in original_cols]\n",
    "    print(f\"  New features: {new_cols}\")\n",
    "    print(f\"  Total features: {len(X_train_fe.columns)}\")\n",
    "    \n",
    "    X_train_fe.to_csv(X_train_out.path, index=False)\n",
    "    X_test_fe.to_csv(X_test_out.path, index=False)\n",
    "    \n",
    "    print(f\"  âœ… Feature engineering completed\")\n",
    "    \n",
    "    return len(new_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 ëª¨ë¸ í•™ìŠµ ì»´í¬ë„ŒíŠ¸ (MLflow ì—°ë™)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.0.3\",\n",
    "        \"scikit-learn==1.3.2\",\n",
    "        \"mlflow==2.9.2\",\n",
    "        \"numpy==1.24.3\"\n",
    "    ]\n",
    ")\n",
    "def train_model(\n",
    "    X_train: Input[Dataset],\n",
    "    X_test: Input[Dataset],\n",
    "    y_train: Input[Dataset],\n",
    "    y_test: Input[Dataset],\n",
    "    mlflow_tracking_uri: str,\n",
    "    experiment_name: str,\n",
    "    team_name: str,\n",
    "    n_estimators: int = 100,\n",
    "    max_depth: int = 10\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ í•™ìŠµ ë° MLflow ê¸°ë¡\n",
    "    \n",
    "    TODO:\n",
    "    1. ë°ì´í„° ë¡œë“œ\n",
    "    2. MLflow ì„¤ì •\n",
    "    3. ëª¨ë¸ í•™ìŠµ\n",
    "    4. ë©”íŠ¸ë¦­ ê¸°ë¡\n",
    "    5. ëª¨ë¸ ì €ì¥\n",
    "    \n",
    "    Returns:\n",
    "        MLflow Run ID\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import mlflow\n",
    "    import mlflow.sklearn\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "    import os\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"  Model Training - {team_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ë°ì´í„° ë¡œë“œ\n",
    "    X_train_df = pd.read_csv(X_train.path)\n",
    "    X_test_df = pd.read_csv(X_test.path)\n",
    "    y_train_df = pd.read_csv(y_train.path)\n",
    "    y_test_df = pd.read_csv(y_test.path)\n",
    "    \n",
    "    # MLflow ì„¤ì •\n",
    "    os.environ['MLFLOW_TRACKING_URI'] = mlflow_tracking_uri\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"{team_name}-run\") as run:\n",
    "        run_id = run.info.run_id\n",
    "        \n",
    "        # íŒŒë¼ë¯¸í„° ë¡œê¹…\n",
    "        params = {\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.set_tag(\"team\", team_name)\n",
    "        \n",
    "        # ëª¨ë¸ í•™ìŠµ\n",
    "        model = RandomForestRegressor(**params)\n",
    "        model.fit(X_train_df, y_train_df.values.ravel())\n",
    "        \n",
    "        # ì˜ˆì¸¡ ë° í‰ê°€\n",
    "        y_pred = model.predict(X_test_df)\n",
    "        \n",
    "        r2 = r2_score(y_test_df, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_df, y_pred))\n",
    "        mae = mean_absolute_error(y_test_df, y_pred)\n",
    "        \n",
    "        # ë©”íŠ¸ë¦­ ë¡œê¹…\n",
    "        mlflow.log_metrics({\"r2\": r2, \"rmse\": rmse, \"mae\": mae})\n",
    "        \n",
    "        print(f\"  R2: {r2:.4f}\")\n",
    "        print(f\"  RMSE: {rmse:.4f}\")\n",
    "        print(f\"  MAE: {mae:.4f}\")\n",
    "        \n",
    "        # ëª¨ë¸ ì €ì¥\n",
    "        mlflow.sklearn.log_model(\n",
    "            model,\n",
    "            \"model\",\n",
    "            registered_model_name=f\"{team_name}-california-model\"\n",
    "        )\n",
    "        \n",
    "        print(f\"  âœ… Run ID: {run_id}\")\n",
    "    \n",
    "    return run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 ëª¨ë¸ í‰ê°€ ì»´í¬ë„ŒíŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"mlflow==2.9.2\"]\n",
    ")\n",
    "def evaluate_model(\n",
    "    run_id: str,\n",
    "    mlflow_tracking_uri: str,\n",
    "    r2_threshold: float = 0.75\n",
    ") -> str:\n",
    "    \"\"\"ëª¨ë¸ í‰ê°€ ë° ë°°í¬ ê²°ì •\"\"\"\n",
    "    import mlflow\n",
    "    import os\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"  Model Evaluation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    os.environ['MLFLOW_TRACKING_URI'] = mlflow_tracking_uri\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "    \n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    run = client.get_run(run_id)\n",
    "    \n",
    "    r2 = float(run.data.metrics.get(\"r2\", 0))\n",
    "    \n",
    "    print(f\"  Run ID: {run_id}\")\n",
    "    print(f\"  R2 Score: {r2:.4f}\")\n",
    "    print(f\"  Threshold: {r2_threshold}\")\n",
    "    \n",
    "    if r2 >= r2_threshold:\n",
    "        decision = \"deploy\"\n",
    "        print(f\"  âœ… Decision: DEPLOY\")\n",
    "    else:\n",
    "        decision = \"skip\"\n",
    "        print(f\"  âš ï¸ Decision: SKIP\")\n",
    "    \n",
    "    return decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 ë°°í¬ / ì•Œë¦¼ ì»´í¬ë„ŒíŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"kubernetes==28.1.0\"]\n",
    ")\n",
    "def deploy_model(run_id: str, model_name: str, namespace: str):\n",
    "    \"\"\"KServe ë°°í¬\"\"\"\n",
    "    from kubernetes import client, config\n",
    "    from kubernetes.client.rest import ApiException\n",
    "    import time\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"  Model Deployment\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"  Model: {model_name}\")\n",
    "    print(f\"  Namespace: {namespace}\")\n",
    "    print(f\"  Run ID: {run_id}\")\n",
    "    \n",
    "    try:\n",
    "        config.load_incluster_config()\n",
    "    except:\n",
    "        config.load_kube_config()\n",
    "    \n",
    "    api = client.CustomObjectsApi()\n",
    "    \n",
    "    isvc = {\n",
    "        \"apiVersion\": \"serving.kserve.io/v1beta1\",\n",
    "        \"kind\": \"InferenceService\",\n",
    "        \"metadata\": {\"name\": model_name, \"namespace\": namespace},\n",
    "        \"spec\": {\n",
    "            \"predictor\": {\n",
    "                \"sklearn\": {\n",
    "                    \"storageUri\": f\"mlflow-artifacts:/{run_id}/model\",\n",
    "                    \"resources\": {\n",
    "                        \"requests\": {\"cpu\": \"100m\", \"memory\": \"256Mi\"},\n",
    "                        \"limits\": {\"cpu\": \"500m\", \"memory\": \"512Mi\"}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        api.delete_namespaced_custom_object(\n",
    "            group=\"serving.kserve.io\", version=\"v1beta1\",\n",
    "            namespace=namespace, plural=\"inferenceservices\", name=model_name\n",
    "        )\n",
    "        time.sleep(5)\n",
    "    except ApiException:\n",
    "        pass\n",
    "    \n",
    "    api.create_namespaced_custom_object(\n",
    "        group=\"serving.kserve.io\", version=\"v1beta1\",\n",
    "        namespace=namespace, plural=\"inferenceservices\", body=isvc\n",
    "    )\n",
    "    \n",
    "    print(f\"  âœ… InferenceService created\")\n",
    "    print(f\"  Endpoint: http://{model_name}.{namespace}.svc.cluster.local/v1/models/{model_name}:predict\")\n",
    "\n",
    "\n",
    "@component(base_image=\"python:3.9-slim\")\n",
    "def send_alert(run_id: str, team_name: str):\n",
    "    \"\"\"ì•Œë¦¼ ì „ì†¡\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"  Alert - {team_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"  âš ï¸ Model did not meet threshold\")\n",
    "    print(f\"  Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. íŒŒì´í”„ë¼ì¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"Project Pipeline\",\n",
    "    description=\"Team Project: E2E ML Pipeline\"\n",
    ")\n",
    "def project_pipeline(\n",
    "    dataset_name: str = \"california\",\n",
    "    team_name: str = \"team-01\",\n",
    "    experiment_name: str = \"team-01-experiment\",\n",
    "    model_name: str = \"team-01-model\",\n",
    "    namespace: str = \"kubeflow-user01\",\n",
    "    mlflow_tracking_uri: str = \"http://mlflow-server-service.mlflow-system.svc.cluster.local:5000\",\n",
    "    n_estimators: int = 100,\n",
    "    max_depth: int = 10,\n",
    "    r2_threshold: float = 0.75\n",
    "):\n",
    "    \"\"\"í”„ë¡œì íŠ¸ íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "    \n",
    "    # Step 1: ë°ì´í„° ë¡œë“œ\n",
    "    load_task = load_data(dataset_name=dataset_name)\n",
    "    \n",
    "    # Step 2: ì „ì²˜ë¦¬\n",
    "    preprocess_task = preprocess(input_data=load_task.outputs[\"output_data\"])\n",
    "    \n",
    "    # Step 3: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
    "    feature_task = feature_engineering(\n",
    "        X_train_in=preprocess_task.outputs[\"X_train_out\"],\n",
    "        X_test_in=preprocess_task.outputs[\"X_test_out\"]\n",
    "    )\n",
    "    \n",
    "    # Step 4: ëª¨ë¸ í•™ìŠµ\n",
    "    train_task = train_model(\n",
    "        X_train=feature_task.outputs[\"X_train_out\"],\n",
    "        X_test=feature_task.outputs[\"X_test_out\"],\n",
    "        y_train=preprocess_task.outputs[\"y_train_out\"],\n",
    "        y_test=preprocess_task.outputs[\"y_test_out\"],\n",
    "        mlflow_tracking_uri=mlflow_tracking_uri,\n",
    "        experiment_name=experiment_name,\n",
    "        team_name=team_name,\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth\n",
    "    )\n",
    "    \n",
    "    # Step 5: í‰ê°€\n",
    "    evaluate_task = evaluate_model(\n",
    "        run_id=train_task.output,\n",
    "        mlflow_tracking_uri=mlflow_tracking_uri,\n",
    "        r2_threshold=r2_threshold\n",
    "    )\n",
    "    \n",
    "    # Step 6: ì¡°ê±´ë¶€ ë°°í¬\n",
    "    with dsl.If(evaluate_task.output == \"deploy\"):\n",
    "        deploy_model(\n",
    "            run_id=train_task.output,\n",
    "            model_name=model_name,\n",
    "            namespace=namespace\n",
    "        )\n",
    "    \n",
    "    with dsl.If(evaluate_task.output == \"skip\"):\n",
    "        send_alert(run_id=train_task.output, team_name=team_name)\n",
    "\n",
    "print(\"âœ… íŒŒì´í”„ë¼ì¸ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. íŒŒì´í”„ë¼ì¸ ì»´íŒŒì¼ ë° ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì»´íŒŒì¼\n",
    "pipeline_file = f'{TEAM_NAME}_pipeline.yaml'\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=project_pipeline,\n",
    "    package_path=pipeline_file\n",
    ")\n",
    "\n",
    "print(f\"âœ… Pipeline compiled: {pipeline_file}\")\n",
    "print(f\"\\në‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(f\"  1. Kubeflow UI â†’ Pipelines â†’ Upload pipeline\")\n",
    "print(f\"  2. {pipeline_file} íŒŒì¼ ì„ íƒ\")\n",
    "print(f\"  3. Create Run â†’ Parameters ì„¤ì •\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹¤í–‰ (ì„ íƒ)\n",
    "try:\n",
    "    client = kfp.Client()\n",
    "    print(f\"âœ… KFP Client connected\")\n",
    "    \n",
    "    run = client.create_run_from_pipeline_func(\n",
    "        project_pipeline,\n",
    "        arguments={\n",
    "            'dataset_name': 'california',\n",
    "            'team_name': TEAM_NAME,\n",
    "            'experiment_name': f'{TEAM_NAME}-experiment',\n",
    "            'model_name': f'{TEAM_NAME}-model',\n",
    "            'namespace': USER_NAMESPACE,\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 10,\n",
    "            'r2_threshold': 0.75\n",
    "        },\n",
    "        experiment_name=f'{TEAM_NAME}-project',\n",
    "        run_name=f'{TEAM_NAME}-run-001'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… Pipeline submitted!\")\n",
    "    print(f\"   Run ID: {run.run_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not submit: {e}\")\n",
    "    print(\"\\nğŸ’¡ Upload YAML manually via Kubeflow UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "ë°œí‘œ ì „ í™•ì¸ì‚¬í•­:\n",
    "\n",
    "- [ ] íŒŒì´í”„ë¼ì¸ì´ ì˜¤ë¥˜ ì—†ì´ ì»´íŒŒì¼ë˜ëŠ”ê°€?\n",
    "- [ ] ëª¨ë“  ì»´í¬ë„ŒíŠ¸ê°€ ì •ìƒ ì‹¤í–‰ë˜ëŠ”ê°€?\n",
    "- [ ] í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì—ì„œ ìƒˆ í”¼ì²˜ê°€ ì¶”ê°€ë˜ì—ˆëŠ”ê°€?\n",
    "- [ ] MLflowì— ì‹¤í—˜ì´ ê¸°ë¡ë˜ëŠ”ê°€?\n",
    "- [ ] ì¡°ê±´ë¶€ ë¶„ê¸°ê°€ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•˜ëŠ”ê°€?\n",
    "- [ ] (ì„ íƒ) ëª¨ë¸ ë°°í¬ê°€ ì„±ê³µí•˜ëŠ”ê°€?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“‹ ë°œí‘œ ì¤€ë¹„\n",
    "\n",
    "### ë°œí‘œ ë‚´ìš© (15ë¶„)\n",
    "\n",
    "1. **íŒ€ ì†Œê°œ** (1ë¶„)\n",
    "   - íŒ€ì› ë° ì—­í• \n",
    "\n",
    "2. **ì•„í‚¤í…ì²˜** (2ë¶„)\n",
    "   - íŒŒì´í”„ë¼ì¸ êµ¬ì¡°\n",
    "   - ì»´í¬ë„ŒíŠ¸ ê°„ ë°ì´í„° íë¦„\n",
    "\n",
    "3. **êµ¬í˜„ í•˜ì´ë¼ì´íŠ¸** (4ë¶„)\n",
    "   - Feature Engineering ì„¤ëª…\n",
    "   - í•µì‹¬ ì½”ë“œ ì„¤ëª…\n",
    "\n",
    "4. **ë°ëª¨** (4ë¶„)\n",
    "   - Kubeflow UI ê²°ê³¼\n",
    "   - MLflow UI ê²°ê³¼\n",
    "   - (ì„ íƒ) API í…ŒìŠ¤íŠ¸\n",
    "\n",
    "5. **íŠ¸ëŸ¬ë¸”ìŠˆíŒ…** (1ë¶„)\n",
    "   - ê²ªì€ ë¬¸ì œì™€ í•´ê²° ë°©ë²•\n",
    "\n",
    "6. **Q&A** (3ë¶„)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
