# ğŸ““ Jupyter Notebook ì‹¤ìŠµ ê°€ì´ë“œ

## âš ï¸ ì¤‘ìš”: AWS S3 ì—°ê²° ì„¤ì •

**ì‹¤ìŠµ ì‹œì‘ ì „ ë°˜ë“œì‹œ AWS ìê²© ì¦ëª…ì„ ì„¤ì •í•˜ì„¸ìš”!**

```python
# Cell 0: AWS ìê²© ì¦ëª… ì„¤ì • (í•„ìˆ˜!)
import os

# AWS ìê²© ì¦ëª… ì„¤ì • (ì—¬ê¸°ì— ë³¸ì¸ì˜ í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”!)
os.environ['AWS_ACCESS_KEY_ID'] = 'YOUR_ACCESS_KEY'          # ë³€ê²½ í•„ìš”!
os.environ['AWS_SECRET_ACCESS_KEY'] = 'YOUR_SECRET_KEY'      # ë³€ê²½ í•„ìš”!
os.environ['AWS_DEFAULT_REGION'] = 'ap-northeast-2'

# ì—°ê²° í…ŒìŠ¤íŠ¸
import boto3
s3 = boto3.client('s3')

try:
    response = s3.list_buckets()
    print(f"âœ… AWS S3 ì—°ê²° ì„±ê³µ! ë²„í‚· ê°œìˆ˜: {len(response['Buckets'])}")
except Exception as e:
    print(f"âŒ AWS S3 ì—°ê²° ì‹¤íŒ¨: {e}")
    print("\nìê²© ì¦ëª…ì„ í™•ì¸í•˜ì„¸ìš”!")
```

---

## ğŸ“‹ ì‹¤ìŠµ êµ¬ì¡° (ì´ 90ë¶„)

- **Part 1**: ETL Pipeline (45ë¶„)
- **Part 2**: Pandasë¥¼ í™œìš©í•œ Batch Processing (45ë¶„)

---

## ğŸ¯ Part 1: ETL Pipeline (45ë¶„)

### Cell 1: í™˜ê²½ ì„¤ì •

```python
import os
import pandas as pd
import numpy as np
import awswrangler as wr
from datetime import datetime, timedelta

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
USER_NUM = os.getenv('USER_NUM', '01')  # ë³¸ì¸ ë²ˆí˜¸ë¡œ ë³€ê²½!
BUCKET_NAME = f"mlops-training-data-user{USER_NUM}"
AWS_REGION = 'ap-northeast-2'

print(f"ì‚¬ìš©ì: {USER_NUM}")
print(f"ë²„í‚·: {BUCKET_NAME}")
print(f"ë¦¬ì „: {AWS_REGION}")
```

### Cell 2: ETL íŒŒì´í”„ë¼ì¸ ì „ì²´ ì‹¤í–‰

```python
# ETL íŒŒì´í”„ë¼ì¸ ì „ì²´ ì‹¤í–‰
%run scripts/1_etl_pipeline/etl_pipeline.py
```

**ë˜ëŠ” ë‹¨ê³„ë³„ë¡œ ì‹¤í–‰:**

```python
# Cell 2-1: Data Lake êµ¬ì¡° ìƒì„±
import boto3

s3_client = boto3.client('s3', region_name=AWS_REGION)

try:
    # S3 ë²„í‚· ìƒì„±
    s3_client.create_bucket(
        Bucket=BUCKET_NAME,
        CreateBucketConfiguration={'LocationConstraint': AWS_REGION}
    )
    print(f"âœ… ë²„í‚· ìƒì„± ì™„ë£Œ: {BUCKET_NAME}")
except s3_client.exceptions.BucketAlreadyOwnedByYou:
    print(f"âœ… ë²„í‚·ì´ ì´ë¯¸ ì¡´ì¬í•¨: {BUCKET_NAME}")

print("\nğŸ“ Data Lake êµ¬ì¡°:")
print(f"Bronze: s3://{BUCKET_NAME}/raw/")
print(f"Silver: s3://{BUCKET_NAME}/processed/")
print(f"Gold: s3://{BUCKET_NAME}/curated/")
```

```python
# Cell 2-2: ìƒ˜í”Œ ë°ì´í„° ìƒì„±
np.random.seed(42)

num_customers = 1000
customer_ids = list(range(1, num_customers + 1))
names = [f"Customer_{i}" for i in customer_ids]
ages = np.random.randint(18, 70, num_customers)
emails = [f"user{i}@example.com" for i in customer_ids]
cities = np.random.choice(['Seoul', 'Busan', 'Incheon', 'Daegu'], num_customers)
join_dates = [datetime.now() - timedelta(days=np.random.randint(1, 365)) for _ in range(num_customers)]

# ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ì¶”ê°€ (10%)
issue_indices = np.random.choice(num_customers, size=100, replace=False)

# Null ê°’ (33ê°œ)
for idx in issue_indices[:33]:
    emails[idx] = None

# ì¤‘ë³µ (33ê°œ)
for idx in issue_indices[33:66]:
    customer_ids[idx] = customer_ids[0]

# ì˜ëª»ëœ í˜•ì‹ (34ê°œ)
for idx in issue_indices[66:]:
    emails[idx] = f"invalid_{idx}"

df_customers = pd.DataFrame({
    'customer_id': customer_ids,
    'name': names,
    'age': ages,
    'email': emails,
    'city': cities,
    'join_date': join_dates
})

print(f"âœ… ìƒì„± ì™„ë£Œ: {len(df_customers)}ëª…")
print(f"Null: {df_customers['email'].isnull().sum()}")
print(f"ì¤‘ë³µ: {df_customers['customer_id'].duplicated().sum()}")
df_customers.head()
```

---

## ğŸ¯ Part 2: Batch Processing (45ë¶„)

### Cell 3: Batch Processing ì‹¤í–‰

```python
# Batch Processing ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
%run scripts/2_batch_processing/pandas_batch_job.py
```

**ì˜ˆìƒ ì¶œë ¥:**
```
============================================================
BATCH ë°ì´í„° ì²˜ë¦¬ (Pandas)
============================================================
...
âœ… BATCH ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ!
```

### Cell 4: ê²°ê³¼ í™•ì¸

```python
# Gold Layer ê²°ê³¼ ì½ê¸°
gold_path = f"s3://{BUCKET_NAME}/curated/analysis/"

# ë„ì‹œë³„ ë¶„ì„
city_df = wr.s3.read_parquet(gold_path + "city_analysis/")
print("ğŸ“Š ë„ì‹œë³„ ê³ ê° ìˆ˜:")
print(city_df.sort_values('count', ascending=False))
print()

# ë‚˜ì´ëŒ€ë³„ ë¶„ì„
age_df = wr.s3.read_parquet(gold_path + "age_analysis/")
print("ğŸ“Š ë‚˜ì´ëŒ€ë³„ ë¶„í¬:")
print(age_df.sort_values('age_group'))
print()

# í†µê³„ ìš”ì•½
stats_df = wr.s3.read_parquet(gold_path + "statistics/")
print("ğŸ“Š í†µê³„ ìš”ì•½:")
print(stats_df)
```

### Cell 5: ì‹œê°í™” (ì„ íƒì‚¬í•­) - ì˜ì–´ ë¼ë²¨ë§Œ ì‚¬ìš©

```python
import matplotlib.pyplot as plt

# âš ï¸ ê·¸ë˜í”„ ë¼ë²¨ì€ ì˜ì–´ë§Œ ì‚¬ìš© (í•œê¸€ í°íŠ¸ ì´ìŠˆ ë°©ì§€)
plt.rcParams['axes.unicode_minus'] = False

# ì°¨íŠ¸ ìƒì„±
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# 1. ë„ì‹œë³„ ê³ ê° ìˆ˜
city_df_plot = city_df.copy()
axes[0].bar(city_df_plot['city'], city_df_plot['count'], color='steelblue', alpha=0.8)
axes[0].set_title('Customer Count by City', fontsize=13, fontweight='bold', pad=15)
axes[0].set_xlabel('City', fontsize=11)
axes[0].set_ylabel('Count', fontsize=11)
axes[0].grid(axis='y', alpha=0.3, linestyle='--')

# ê°’ ë ˆì´ë¸” ì¶”ê°€
for i, (city, count) in enumerate(zip(city_df_plot['city'], city_df_plot['count'])):
    axes[0].text(i, count + 10, str(count), ha='center', va='bottom', fontsize=10)

# 2. ë‚˜ì´ëŒ€ë³„ ë¶„í¬
age_df_plot = age_df.copy()
axes[1].bar(age_df_plot['age_group'], age_df_plot['count'], color='coral', alpha=0.8)
axes[1].set_title('Customer Distribution by Age Group', fontsize=13, fontweight='bold', pad=15)
axes[1].set_xlabel('Age Group', fontsize=11)
axes[1].set_ylabel('Count', fontsize=11)
axes[1].grid(axis='y', alpha=0.3, linestyle='--')

# ê°’ ë ˆì´ë¸” ì¶”ê°€
for i, (age, count) in enumerate(zip(age_df_plot['age_group'], age_df_plot['count'])):
    axes[1].text(i, count + 5, str(count), ha='center', va='bottom', fontsize=10)

plt.tight_layout()
plt.show()

print("\nâœ… ì°¨íŠ¸ ìƒì„± ì™„ë£Œ!")
print("   - ì™¼ìª½: ë„ì‹œë³„ ê³ ê° ìˆ˜")
print("   - ì˜¤ë¥¸ìª½: ë‚˜ì´ëŒ€ë³„ ë¶„í¬")
```

---

## ğŸ’¡ ì™œ Pandasë¥¼ ì‚¬ìš©í•˜ë‚˜ìš”?

### âœ… Pandasì˜ ì¥ì 
1. **ê°„ë‹¨í•˜ê³  ì§ê´€ì ** - ì¶”ê°€ ì¸í”„ë¼ ë¶ˆí•„ìš”
2. **ë¹ ë¥¸ ê°œë°œ** - ë³µì¡í•œ ì„¤ì • ì—†ìŒ
3. **ì‰¬ìš´ ë””ë²„ê¹…** - ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
4. **AWS í†µí•©** - AWS Wranglerì™€ ì™„ë²½í•œ í˜¸í™˜

### âš ï¸ Sparkê°€ í•„ìš”í•œ ê²½ìš°
- 10GB ì´ìƒì˜ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
- ë³µì¡í•œ ë¶„ì‚° ì²˜ë¦¬ ì‘ì—…
- ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬

**ì´ë²ˆ ì‹¤ìŠµ(1000í–‰)ì—ì„œëŠ” Pandasê°€ ìµœì ì˜ ì„ íƒì…ë‹ˆë‹¤!**

---

## ğŸ‰ ì‹¤ìŠµ ì™„ë£Œ!

ì¶•í•˜í•©ë‹ˆë‹¤! ë‹¤ìŒì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤:

1. âœ… **Part 1**: S3 Data Lake + ETL Pipeline
2. âœ… **Part 2**: Pandas ê¸°ë°˜ Batch Processing

### ë‹¤ìŒ ë‹¨ê³„
- **Day 2**: ëª¨ë¸ ì„œë¹™ & ë²„ì „ ê´€ë¦¬
- **Lab 2-1**: FastAPI ëª¨ë¸ ì„œë¹™
- **Lab 2-2**: MLflow Tracking & Registry

---

Â© 2025 í˜„ëŒ€ì˜¤í† ì—ë²„ MLOps Training
